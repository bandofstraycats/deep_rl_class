{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Deep reinforcement learning to play Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q learning](fig/q-learning.png)\n",
    "[source](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html)\n",
    "\n",
    "Influential paper: [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "* Random agent\n",
    "* DQN agent\n",
    "* Exploration / Experience replay \n",
    "* Double DQN\n",
    "* Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### OpenAI Gym\n",
    "OpenAI Gym is library that allows to train agents in a wide variety of environments with near-identical interface.\n",
    "#### Installation\n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```\n",
    "### Keras\n",
    "Keras is a high-level library on the top of deep learning backends (Tensorflow, Theano)\n",
    "#### Installation\n",
    "```\n",
    "pip install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, Reshape, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we consider the CartPole problem where the objective is to balance the pole by applying force to the cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-03 21:13:36,774] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f252da21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole'\n",
    "env = gym.make(\"{}-v0\".format(ENV_NAME))\n",
    "env.reset()\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CartPole, we observe 4 variables: position of cart, its velocity, angle of pole, and angular velocity. At any state, there are only has two possible actions: move to the left or move to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym API key elements\n",
    "* env = gym.make(environment_name) <- sets up the environment\n",
    "* env.reset() <- resets the environment to starting point\n",
    "* env.step(action) <- takes action and goes to state $S_{t+1}$, returns $r_t$, $S_{t+1}$ and if game terminated.\n",
    "* env.render() <- render the output\n",
    "\n",
    "## Tensorboard logging\n",
    "*Optional* - If your backend is Tensorflow you can observe some parameters during the training using the board\n",
    "```\n",
    "tensorboard --logdir=/tmp/logs --port=8000\n",
    "```\n",
    "but you need to set callbacks in the code \n",
    "```\n",
    "tensorboard_cback = keras.callbacks.TensorBoard(log_dir='/tmp/logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "model.fit(...inputs and parameters..., callbacks=[tensorboard_cback])\n",
    "```\n",
    "more details in the documentation https://keras.io/callbacks/#tensorboard and on [Stackoverflow](https://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "\n",
    "The world's simplest agent that independently chooses random actions at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play loop\n",
    "\n",
    "Note that CartPole Gym environment is considered as solved as soon as you find a policy which scores 200 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total reward 13.0\n",
      "Episode 1, Total reward 15.0\n",
      "Episode 2, Total reward 17.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f25ec07160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_policy(env, agent, add_batch_dim=False, render=False, episodes = 1):\n",
    "    done = False\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()\n",
    "        if add_batch_dim: \n",
    "                # This is a technical reshaping because models are predict on a batch\n",
    "                state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            plt.imshow(env.render('rgb_array'))\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if add_batch_dim:\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "            state = next_state\n",
    "            total_reward += reward \n",
    "            if done:\n",
    "                print(\"Episode {}, Total reward {}\".format(i, total_reward))\n",
    "                break       \n",
    "    env.close()\n",
    "    \n",
    "    \n",
    "display_policy(env, RandomAgent(env.action_space), episodes=3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DQN Agent - Online\n",
    "\n",
    "This is a [Keras](https://keras.io/) code for training a Deep Q-Network (DQN) without exploration. \n",
    "\n",
    "We start with the simple DQN agent -- the trained network is immediatly used to collect the new training data. Unless you are *very* lucky you won't be able to solve the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "* Replace the `???` by values that seem reasonnable to you ($\\gamma>1$ is not reasonnable and big steps are prone to numerical instability). \n",
    "* See how the policy training fails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # hyper parameters \n",
    "        self.gamma = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # this is a simple one hidden layer model, but it should be enough for our task\n",
    "        # it is much easier to train with different achitectures (stack layers, change activation)\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='tanh'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.summary()\n",
    "        # 1/ you can try different losses, for ex., logcosh loss is a twice differenciable approximation of Huber loss\n",
    "        # 2/ from a theoretical perspective learning rate should decay with time to guarantee convergence \n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # get action from model using greedy policy\n",
    "    def get_action(self, state):\n",
    "        q_value = self.model.predict(state)\n",
    "        return np.argmax(q_value[0]) # the [0] is because keras outputs a set of predictions of size 1\n",
    "\n",
    "    # train the target network on the selected action and transition\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        target = self.model.predict(state)\n",
    "        # we use our internal model in order to estimate the V value of the next state \n",
    "        target_val = self.model.predict(next_state)\n",
    "        # Q Learning: target values should respect the Bellman's optimality principle\n",
    "        if done: # we are on a terminal state\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.gamma * (np.amax(target_val))\n",
    "\n",
    "        # and do the model fit!\n",
    "        (self.model.train_on_batch(state, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 1, Total reward 10.0\n",
      "Episode 11, Total reward 10.0\n",
      "Episode 21, Total reward 9.0\n",
      "Episode 31, Total reward 9.0\n",
      "Episode 41, Total reward 10.0\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DQNAgent(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 50\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        agent.train_model(action, state, next_state, reward, done)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 10 == 1:\n",
    "                print(\"Episode {}, Total reward {}\".format(i, total_reward))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration\n",
    "\n",
    "This is the first agent that is going to solve the task. It will typically require to run a few hundred episodes to collect the data. \n",
    "\n",
    "The difference with the previous agent is that we add an exploration mechanism in order to take care of the data collection for the training. We advise to use an $\\varepsilon_n$-greedy, meaning that the value of $\\varepsilon$ is going to decay over time. Several kind of decays can be found in the litterature. A simple one is to use a mutiplicative update of $\\varepsilon$ by a constant smaller than 1 as long as $\\varepsilon$ is smaller than a small minimal rate (typically, in the range 1%-5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "* Code your exploration marked by TODOs\n",
    "* Tune the hyperparameters (including the ones from the previous section) in order to solve the task. This may be not so easy and will likely require more than 500 episodes and a final small value of epsilon. Next sessions will be about techniques to increase sample efficiency (i.e require less episodes).\n",
    "\n",
    "**Trouble shooting:** *some working parameters and network architecture are provided at the bottom of this page in case you have trouble finding a working situation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExploration(DQNAgent):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgentWithExploration, self).__init__(state_size, action_size)\n",
    "        self.epsilon = 1.0\n",
    "        # exploration schedule parameters \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        # TODO: store your additional parameters here \n",
    "\n",
    "    # decay epsilon\n",
    "    def update_epsilon(self):\n",
    "        # TODO: write the code for your decay  \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    # get action from model using greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            # TODO: add the exploration \n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0, Total reward 16.0, Epsilon 0.92\n",
      "Episode 20, Total reward 10.0, Epsilon 0.28\n",
      "Episode 40, Total reward 10.0, Epsilon 0.10\n",
      "Episode 60, Total reward 10.0, Epsilon 0.04\n",
      "Episode 80, Total reward 9.0, Epsilon 0.01\n",
      "Episode 100, Total reward 10.0, Epsilon 0.01\n",
      "Episode 120, Total reward 10.0, Epsilon 0.01\n",
      "Episode 140, Total reward 12.0, Epsilon 0.01\n",
      "Episode 160, Total reward 16.0, Epsilon 0.01\n",
      "Episode 180, Total reward 11.0, Epsilon 0.01\n",
      "Episode 200, Total reward 11.0, Epsilon 0.01\n",
      "Episode 220, Total reward 15.0, Epsilon 0.01\n",
      "Episode 240, Total reward 12.0, Epsilon 0.01\n",
      "Episode 260, Total reward 12.0, Epsilon 0.01\n",
      "Episode 280, Total reward 13.0, Epsilon 0.01\n",
      "Episode 300, Total reward 20.0, Epsilon 0.01\n",
      "Episode 320, Total reward 18.0, Epsilon 0.01\n",
      "Episode 340, Total reward 19.0, Epsilon 0.01\n",
      "Episode 360, Total reward 16.0, Epsilon 0.01\n",
      "Episode 380, Total reward 20.0, Epsilon 0.01\n",
      "Episode 400, Total reward 11.0, Epsilon 0.01\n",
      "Episode 420, Total reward 12.0, Epsilon 0.01\n",
      "Episode 440, Total reward 37.0, Epsilon 0.01\n",
      "Episode 460, Total reward 20.0, Epsilon 0.01\n",
      "Episode 480, Total reward 27.0, Epsilon 0.01\n",
      "Episode 500, Total reward 26.0, Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DQNAgentWithExploration(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 501\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        agent.train_model(action, state, next_state, reward, done)\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {}, Total reward {}, Epsilon {:0.2f}\".format(i, total_reward, agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "We are now going to save some samples in a limited memory in order to assemble mini-batches during the training. The exploration policy remains the same as in the previous section.  \n",
    "\n",
    "Reference: [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "* Fill the TODO section that assembles the mini-batch sent to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DQNAgentWithExplorationAndReplay(DQNAgentWithExploration):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgentWithExplorationAndReplay, self).__init__(state_size, action_size)\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 64\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=5000)\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))        \n",
    "\n",
    "    \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        # pick samples randomly from replay memory (using batch_size)\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # TODO\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (\n",
    "                        np.amax(target_val[i])) # target_val[i][np.argmax(target_val[i])])\n",
    "           \n",
    "        # and do the model fit!\n",
    "        (\"loss :\", self.model.train_on_batch(update_input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0,\t Total reward 66.0,\t Memory length 66,\t Epsilon 0.72\n",
      "Episode 20,\t Total reward 26.0,\t Memory length 417,\t Epsilon 0.12\n",
      "Episode 40,\t Total reward 25.0,\t Memory length 787,\t Epsilon 0.02\n",
      "Episode 60,\t Total reward 29.0,\t Memory length 1462,\t Epsilon 0.01\n",
      "Episode 80,\t Total reward 120.0,\t Memory length 3092,\t Epsilon 0.01\n",
      "Episode 100,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 120,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 140,\t Total reward 108.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 160,\t Total reward 121.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 180,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 200,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 220,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 240,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 260,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 280,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 300,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DQNAgentWithExplorationAndReplay(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 301\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        agent.train_model()\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {},\\t Total reward {},\\t Memory length {},\\t Epsilon {:0.2f}\".format(i, total_reward, len(agent.memory), agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "Now we train two identical networks. We keep frozen for some timesteps the network that is in charge of evaluation (i.e that is used to compute targets).\n",
    "Note that you can find some variants where the target network is updated at each timestep but with a small fraction of the difference with the policy network.\n",
    "\n",
    "Reference: [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "* Implement Double DQN, this is should reduce the variance of predicted Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgentWithExplorationAndReplay(DQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DoubleDQNAgentWithExplorationAndReplay, self).__init__(state_size, action_size)\n",
    "        # TODO: initialize a second model\n",
    "        self.target_model = self.build_model()\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # TODO: copy weights from the model used for action selection to the model used for computing targets\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        # pick samples randomly from replay memory \n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "        # TODO: compute targets (in target_val)        \n",
    "        double_target_val = self.target_model.predict(update_target)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # approx Q Learning\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (double_target_val[i][np.argmax(target_val[i])])\n",
    "          \n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.train_on_batch(update_input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_27 (Dense)             (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0,\t Total reward 17.0,\t Memory length 17,\t Epsilon 0.92\n",
      "Episode 20,\t Total reward 8.0,\t Memory length 289,\t Epsilon 0.23\n",
      "Episode 40,\t Total reward 47.0,\t Memory length 585,\t Epsilon 0.05\n",
      "Episode 60,\t Total reward 33.0,\t Memory length 1473,\t Epsilon 0.01\n",
      "Episode 80,\t Total reward 39.0,\t Memory length 2274,\t Epsilon 0.01\n",
      "Episode 100,\t Total reward 57.0,\t Memory length 3261,\t Epsilon 0.01\n",
      "Episode 120,\t Total reward 171.0,\t Memory length 4841,\t Epsilon 0.01\n",
      "Episode 140,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 160,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 180,\t Total reward 199.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 200,\t Total reward 192.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 220,\t Total reward 171.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 240,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 260,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 280,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 300,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DoubleDQNAgentWithExplorationAndReplay(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 301\n",
    "done = False\n",
    "\n",
    "# TODO: add the update of the target model at the end of each episode\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        agent.train_model()\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {},\\t Total reward {},\\t Memory length {},\\t Epsilon {:0.2f}\".format(i, total_reward, len(agent.memory), agent.epsilon))\n",
    "            break    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the actual performance of the policy, we should set $\\varepsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total reward 200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f26175a518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.epsilon = 0\n",
    "display_policy(env, agent, episodes=1, add_batch_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dueling DQN \n",
    "\n",
    "Dueling network contains two separate estimators: one for the state value function and one for the state-dependent action advantage function. This should reduce the variance of Q-values estimation.\n",
    "\n",
    "Reference: [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)\n",
    "\n",
    "## Task 5\n",
    "\n",
    "* Implement Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DuelingDoubleDQNAgentWithExplorationAndReplay(DoubleDQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DuelingDoubleDQNAgentWithExplorationAndReplay, self).__init__(state_size, action_size)\n",
    "        self.model = build_dueling_model()\n",
    "        self.target_model = build_dueling_model()\n",
    "        \n",
    "    def build_dueling_model(self):\n",
    "        inputs = Input(shape=self.state_size)\n",
    "        net = Dense(128, input_dim=self.state_size, activation='tanh')(inputs)\n",
    "        \n",
    "        y = Dense(self.action_size + 1, activation='linear')(net)\n",
    "        final = Lambda(lambda a: K.expand_dims(a[:, 0], -1) + a[:, 1:] - K.mean(a[:, 1:], keepdims=True),\n",
    "                       output_shape=(self.action_size,))(y)\n",
    "        \n",
    "        model = Model(inputs=inputs, outputs=final)\n",
    "        model.summary()\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_33 (Dense)             (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 898\n",
      "Trainable params: 898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 0,\t Total reward 14.0,\t Memory length 14,\t Epsilon 0.93\n",
      "Episode 20,\t Total reward 14.0,\t Memory length 504,\t Epsilon 0.08\n",
      "Episode 40,\t Total reward 39.0,\t Memory length 1078,\t Epsilon 0.01\n",
      "Episode 60,\t Total reward 51.0,\t Memory length 1884,\t Epsilon 0.01\n",
      "Episode 80,\t Total reward 45.0,\t Memory length 3016,\t Epsilon 0.01\n",
      "Episode 100,\t Total reward 91.0,\t Memory length 4862,\t Epsilon 0.01\n",
      "Episode 120,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 140,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 160,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 180,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 200,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 220,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 240,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 260,\t Total reward 83.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 280,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 300,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DoubleDQNAgentWithExplorationAndReplay(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 301\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        agent.train_model()\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {},\\t Total reward {},\\t Memory length {},\\t Epsilon {:0.2f}\".format(i, total_reward, len(agent.memory), agent.epsilon))\n",
    "            break    \n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More ideas to try\n",
    "\n",
    "* Compute averaged curves over several initialisations for each algorithm\n",
    "* Shape the reward in order to keep the cart close to its initial position\n",
    "* Do not sample uniformly in the memory for the batches using [prioritized replay](https://arxiv.org/pdf/1511.05952.pdf)\n",
    "* (Painfull) learn a policy with a pole starting from the bottom position\n",
    "* [Visualize](http://arxiv.org/abs/1412.6806) important areas of the pictures for taking the decision using saliency maps computed by guided backpropagation\n",
    "* Try [policy gradient method](http://www.scholarpedia.org/article/Policy_gradient_methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoiler\n",
    "\n",
    "Some working parameters for the network: \n",
    "* gamma = 0.995\n",
    "* learning_rate = 0.001\n",
    "* Exploration by $\\varepsilon$-greddy with multiplicative decay and minimal value \n",
    "  ```\n",
    "  self.epsilon = 1.0\n",
    "  self.epsilon_decay = 0.995\n",
    "  self.epsilon_min = 0.01\n",
    "  ```\n",
    "* Structure of the network: 1 layer with 128 hidden units with tanh activation, MSE loss and optimized using Adam optimizer:\n",
    "  ```\n",
    "  model.add(Dense(128, input_dim=self.state_size, activation='tanh'))\n",
    "  model.add(Dense(self.action_size, activation='linear'))\n",
    "  model.summary()\n",
    "  model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "  ```        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
