{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Reinforcement Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "<img src=\"fig/rl.png\" width=\"300\" alt=\"RL Setup\"/>\n",
    "\n",
    "\n",
    "Markov Decision Process $M := (\\mathcal{S}, \\mathcal{A}, P, r)$ \n",
    "\n",
    "$\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the finite action space\n",
    "\n",
    "$P$ is the transition probability kernel\n",
    "\n",
    "$r(s,a)$ is the reward function mapping state-action pairs to real-valued subset\n",
    "\n",
    "Stochastic policy $\\pi$ as probability distribution over actions given a state $\\pi(a|s)$\n",
    "\n",
    "### Goal\n",
    "\n",
    "Maximise w.r.t $\\pi$ the expected discounted total reward:\n",
    "    \n",
    "$R = \\sum \\limits_{t=0}^{\\infty} \\gamma^t r_t$\n",
    "\n",
    "$\\gamma$ is the discount factor that trades-off the importance of immediate and future rewards, $0<\\gamma<1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning for a Markov Decision Process\n",
    "\n",
    "*Action-value function* $Q^{\\pi}$ of a policy $\\pi$: \n",
    "$Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{a \\sim \\pi, s \\sim P} [\\sum_{l=0}^{\\infty} \\gamma^l r_{t+l} | s_t, a_t]$\n",
    "\n",
    "The $Q$ values under policy $\\pi$ represent the total discounted return from taking a given action at a given state and following the policy onwards.\n",
    "\n",
    "*State-value function* $V^{\\pi}$ of a policy $\\pi$: \n",
    "$V^{\\pi}(s_t) = \\mathbb{E}_{a \\sim \\pi} Q^{\\pi}(s_t, a)$\n",
    "\n",
    "The $V$ value of a state under policy $\\pi$ is the expected total discounted return obtained by following $\\pi$ from that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "\n",
    "Given $Q$ function, the agent knows which action is best in a given state\n",
    "\n",
    "Greedy policy is the policy that takes action with the highest $Q$, $\\pi^* = argmax_a Q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman equation\n",
    "\n",
    "$Q$ value and $V$ value can be decomposed using *Bellman equation*:\n",
    "\n",
    "$Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi} [r_t + \\gamma Q^{\\pi}(s_{t+1},a_{t+1}) | s_t=s, a_t=a]$\n",
    "\n",
    "$V^{\\pi}(s) = \\mathbb{E}_{\\pi} [r_t + \\gamma V^{\\pi}(s_{t+1}) | s_t=s]$    \n",
    "\n",
    "Note: after time $t$, actions are chosen by $\\pi$ so in expectation we could replace $Q^{\\pi}(s_{t+1}, a_{t+1})$ by $V^{\\pi}(s_{t+1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation\n",
    "    \n",
    "The optimal policy $\\pi^*$ must satisfy\n",
    "\n",
    "$Q^{\\pi^*}(s,a) = \\mathbb{E}_{\\pi^*} [r_t + \\gamma \\max_a Q^{\\pi^*}(s_{t+1},a)]$\n",
    "\n",
    "**Algorithm**: \n",
    "While the equality does not hold, alternate between two steps:    \n",
    "    \n",
    "1. Policy Evaluation: estimate $Q$ function for a fixed $\\pi$\n",
    "2. Policy Improvement: improve $\\pi$ by acting greedily on $Q$\n",
    "   \n",
    "<img src=\"fig/evalimprov.png\" width=\"480\" alt=\"Policy Evaluation - Policy Improvement\"/>\n",
    "\n",
    "\n",
    "Bootstraping: use your $Q$ values estimation to compute *target*, i.e. right-hand side of Bellman equation\n",
    "\n",
    "Guaranteed to converge in tabular case (contraction argument), but not in approximation case\n",
    "\n",
    "source: [R. S. Sutton, Reinforcement Learning: An introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning (DQN)\n",
    "\n",
    "Approximate $Q$ function using a neural network:\n",
    "\n",
    "$Q^{\\pi^*}(s,a) = \\mathbb{E}_{\\pi^*} [r_t + \\gamma \\max_a Q^{\\pi^*}(s_{t+1},a)]$\n",
    "\n",
    "* $Q$-value is estimated using a neural network\n",
    "* $Q$ network is trained over mini-batches of state-action pairs\n",
    "* Expectation can be approximated using Monte Carlo sampling (in practice, just one sample)\n",
    "\n",
    "Reference: [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience Replay\n",
    "\n",
    "The training dataset of state-action pairs is crucial for the performance\n",
    "\n",
    "* Collect data using a fixed policy with an *exploration mechanism*\n",
    "* Collected samples are highly correlated. To mitigate this, experience replay is an idea to keep a common memory over several episodes and randomly sample batches from there\n",
    "* In prioritized experienced replay, sampling is weighted to replay important transitions more frequently\n",
    "\n",
    "Reference: [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Deep Q Learning (DDQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: overestimation bias of Q-values \n",
    "$\\mathbb{E}_{\\epsilon}[\\max_a (Q(s,a) + \\epsilon)] \\geq \\max_a Q(s,a)$ \n",
    "\n",
    "**Idea**: use two independent estimators $Q_1$ and $Q_2$ to make unbiased value estimates\n",
    "\n",
    "**In practice**: In Double DQN *target network* is used to estimate the right-hand side of Bellman update. Policy is greedy one based on *current network* rather than the target network. Target network is synced with current network every several episodes to simulate \"independent\" estimation.\n",
    "\n",
    "$Q_{1}^{\\pi^*}(s,a) = \\mathbb{E}_{\\pi^*} [r_t + \\gamma \\max_a Q_2^{\\pi^*}(s_{t+1},a)]$\n",
    "\n",
    "Reference: [Deep Reinforcement Learning with Double Q-learning](https://papers.nips.cc/paper/3964-double-q-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling network\n",
    "\n",
    "**Problem**: overestimation bias of Q-values \n",
    "\n",
    "**Idea**: reduce variance of Q-value estimates by subtracting a state value (constant for all actions)\n",
    "\n",
    "* $A(a,s) = Q(s,a) - V(s)$\n",
    "* $A(a,s)$ is called *advantage function*\n",
    "\n",
    "**In practice**: design a neural network that has two flows -- one for $Q$ value estimation and the other one for $V$ value.\n",
    "\n",
    "<img src=\"fig/dueling_network.png\" width=\"480\" alt=\"Dueling Network\"/>\n",
    "\n",
    "Reference: [Dueling Network Architectures for Deep Reinforcement Learning](http://proceedings.mlr.press/v48/wangf16.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic\n",
    "\n",
    "**Problem**: argmax of a greedy policy is not differentiable: small error in $Q$ estimation might result in argmax being arbitrarily far\n",
    "\n",
    "**Idea**: parametrize policy $\\pi = \\pi_{\\theta}(a|s)$, policy improvement step becomes optimization step over $\\theta$\n",
    "\n",
    "**In practice**: two networks -- policy network and value network ($Q$ or $A$ value)\n",
    "\n",
    "* $a \\sim \\pi_{\\theta}(a|s)$\n",
    "* $Q^{\\pi_{\\theta}}(s,a) = \\mathbb{E}_{\\pi_{\\theta}} [r_t + \\gamma \\max_a Q^{\\pi_{\\theta}}(s_{t+1},a)]$\n",
    "* $\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta} \\log \\pi_{\\theta}(a|s)Q^{\\pi_{\\theta}}(s,a)$\n",
    "\n",
    "**Connection to Policy Gradient**: reduce variance of policy gradient by replacing the true reward by its estimated mean, therefore introducing bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "[R. S. Sutton, Reinforcement Learning: An introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf)\n",
    "\n",
    "[Introduction into RL by R. Munos](http://videolectures.net/netadis2013_munos_reinforcement_learning/)\n",
    "\n",
    "[ICML 2017 Deep RL tutorial by S. Levine and C. Finn](https://sites.google.com/view/icml17deeprl)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
