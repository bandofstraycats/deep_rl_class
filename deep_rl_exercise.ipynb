{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using Deep reinforcement learning to play Atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Q learning](fig/q-learning.png)\n",
    "[source](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/deep_q_learning.html)\n",
    "\n",
    "Influential paper: [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "* Random agent\n",
    "* DQN agent\n",
    "* Exploration / Experience replay \n",
    "* Double DQN\n",
    "* Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### OpenAI Gym\n",
    "OpenAI Gym is library that allows to train agents in a wide variety of environments with near-identical interface.\n",
    "#### Installation\n",
    "```\n",
    "git clone https://github.com/openai/gym.git\n",
    "cd gym\n",
    "pip install -e .[all]\n",
    "```\n",
    "### Keras\n",
    "Keras is a high-level library on the top of deep learning backends (Tensorflow, Theano)\n",
    "#### Installation\n",
    "```\n",
    "pip install keras\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.layers import Dense, Conv2D, Flatten, Input, Reshape, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we consider the CartPole problem where the objective is to balance the pole by applying force to the cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-30 11:38:55,222] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x126539f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole'\n",
    "env = gym.make(\"{}-v0\".format(ENV_NAME))\n",
    "env.reset()\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CartPole, we observe 4 variables: position of cart, its velocity, angle of pole, and angular velocity. At any state, there are only has two possible actions: move to the left or move to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym API key elements\n",
    "* env = gym.make(environment_name) <- sets up the environment\n",
    "* env.reset() <- resets the environment to starting point\n",
    "* env.step(action) <- takes action and goes to state $S_{t+1}$, returns $r_t$, $S_{t+1}$ and if game terminated.\n",
    "* env.render() <- render the output\n",
    "\n",
    "## Tensorboard logging\n",
    "*Optional* - If your backend is Tensorflow you can observe some parameters during the training using the board\n",
    "```\n",
    "tensorboard --logdir=/tmp/logs --port=8000\n",
    "```\n",
    "but you need to set callbacks in the code \n",
    "```\n",
    "tensorboard_cback = keras.callbacks.TensorBoard(log_dir='/tmp/logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "model.fit(...inputs and parameters..., callbacks=[tensorboard_cback])\n",
    "```\n",
    "more details in the documentation https://keras.io/callbacks/#tensorboard and on [Stackoverflow](https://stackoverflow.com/questions/42112260/how-do-i-use-the-tensorboard-callback-of-keras) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random agent\n",
    "\n",
    "The world's simplest agent that independently chooses random actions at each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play loop\n",
    "\n",
    "Note that CartPole Gym environment is considered as solved as soon as you find a policy which scores 200 on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total reward 16.0\n",
      "Episode 1, Total reward 19.0\n",
      "Episode 2, Total reward 13.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x8EgKcNw2IrBQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x133c37cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def display_policy(env, agent, add_batch_dim=False, render=False, episodes = 1):\n",
    "    done = False\n",
    "    for i in range(episodes):\n",
    "        state = env.reset()\n",
    "        if add_batch_dim: \n",
    "                # This is a technical reshaping because models are predict on a batch\n",
    "                state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            plt.imshow(env.render('rgb_array'))\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            if add_batch_dim:\n",
    "                next_state = np.reshape(next_state, [1, state_size])\n",
    "            state = next_state\n",
    "            total_reward += reward \n",
    "            if done:\n",
    "                print(\"Episode {}, Total reward {}\".format(i, total_reward))\n",
    "                break       \n",
    "    env.close()\n",
    "    \n",
    "    \n",
    "display_policy(env, RandomAgent(env.action_space), episodes=3)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## DQN Agent - Online\n",
    "\n",
    "This is a [Keras](https://keras.io/) code for training a Deep Q-Network (DQN) without exploration. \n",
    "\n",
    "We start with the simple DQN agent -- the trained network is immediatly used to collect the new training data. Unless you are *very* lucky you won't be able to solve the task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "* Replace the `???` by values that seem reasonnable to you ($\\gamma>1$ is not reasonnable and big steps are prone to numerical instability). \n",
    "* See how the policy training fails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        # hyper parameters \n",
    "        self.gamma = ???\n",
    "        self.learning_rate = ???\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    # approximate Q function using Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        # this is a simple one hidden layer model, but it should be enough for our task\n",
    "        # it is much easier to train with different achitectures (stack layers, change activation)\n",
    "        model.add(Dense(128, input_dim=self.state_size, activation='tanh'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.summary()\n",
    "        # 1/ you can try different losses, for ex., logcosh loss is a twice differenciable approximation of Huber loss\n",
    "        # 2/ from a theoretical perspective learning rate should decay with time to guarantee convergence \n",
    "        model.compile(loss='???', optimizer=???)\n",
    "        return model\n",
    "\n",
    "    # get action from model using greedy policy\n",
    "    def get_action(self, state):\n",
    "        q_value = self.model.predict(state)\n",
    "        return np.argmax(q_value[0]) # the [0] is because keras outputs a set of predictions of size 1\n",
    "\n",
    "    # train the target network on the selected action and transition\n",
    "    def train_model(self, action, state, next_state, reward, done):\n",
    "        target = self.model.predict(state)\n",
    "        # we use our internal model in order to estimate the V value of the next state \n",
    "        target_val = self.model.predict(next_state)\n",
    "        # Q Learning: target values should respect the Bellman's optimality principle\n",
    "        if done: # we are on a terminal state\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.gamma * (np.amax(target_val))\n",
    "\n",
    "        # and do the model fit!\n",
    "        (self.model.train_on_batch(state, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 1, Total reward 9.0\n",
      "Episode 11, Total reward 9.0\n",
      "Episode 21, Total reward 11.0\n",
      "Episode 31, Total reward 10.0\n",
      "Episode 41, Total reward 9.0\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DQNAgent(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 50\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        agent.train_model(action, state, next_state, reward, done)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 10 == 1:\n",
    "                print(\"Episode {}, Total reward {}\".format(i, total_reward))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration\n",
    "\n",
    "This is the first agent that is going to solve the task. It will typically require to run a few hundred episodes to collect the data. \n",
    "\n",
    "The difference with the previous agent is that we add an exploration mechanism in order to take care of the data collection for the training. We advise to use an $\\varepsilon_n$-greedy, meaning that the value of $\\varepsilon$ is going to decay over time. Several kind of decays can be found in the litterature. A simple one is to use a mutiplicative update of $\\varepsilon$ by a constant smaller than 1 as long as $\\varepsilon$ is smaller than a small minimal rate (typically, in the range 1%-5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "* Code your exploration marked by TODOs\n",
    "* Tune the hyperparameters (including the ones from the previous section) in order to solve the task. This may be not so easy and will likely require more than 500 episodes and a final small value of epsilon. Next sessions will be about techniques to increase sample efficiency (i.e require less episodes).\n",
    "\n",
    "**Trouble shooting:** *some working parameters and network architecture are provided at the bottom of this page in case you have trouble finding a working situation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNAgentWithExploration(DQNAgent):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgentWithExploration, self).__init__(state_size, action_size)\n",
    "        # exploration schedule parameters \n",
    "        # TODO: store your additional parameters here \n",
    "\n",
    "    # decay epsilon\n",
    "    def update_epsilon(self):\n",
    "        # TODO: write the code for your decay  \n",
    "\n",
    "    # get action from model using greedy policy\n",
    "    def get_action(self, state):\n",
    "        # TODO: add the exploration \n",
    "        q_value = self.model.predict(state)\n",
    "        return np.argmax(q_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 1, Total reward 32.0, Epsilon 0.76\n",
      "Episode 21, Total reward 11.0, Epsilon 0.20\n",
      "Episode 41, Total reward 10.0, Epsilon 0.07\n",
      "Episode 61, Total reward 9.0, Epsilon 0.03\n",
      "Episode 81, Total reward 10.0, Epsilon 0.01\n",
      "Episode 101, Total reward 9.0, Epsilon 0.01\n",
      "Episode 121, Total reward 8.0, Epsilon 0.01\n",
      "Episode 141, Total reward 10.0, Epsilon 0.01\n",
      "Episode 161, Total reward 11.0, Epsilon 0.01\n",
      "Episode 181, Total reward 10.0, Epsilon 0.01\n",
      "Episode 201, Total reward 10.0, Epsilon 0.01\n",
      "Episode 221, Total reward 9.0, Epsilon 0.01\n",
      "Episode 241, Total reward 9.0, Epsilon 0.01\n",
      "Episode 261, Total reward 9.0, Epsilon 0.01\n",
      "Episode 281, Total reward 10.0, Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DQNAgentWithExploration(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 501\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        total_reward += reward\n",
    "        agent.train_model(action, state, next_state, reward, done)\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {}, Total reward {}, Epsilon {:0.2f}\".format(i, total_reward, agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "We are now going to save some samples in a limited memory in order to assemble mini-batches during the training. The exploration policy remains the same as in the previous section.  \n",
    "\n",
    "Reference: [Human-level control through deep reinforcement learning](https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "* Fill the TODO section that assembles the mini-batch sent to the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class DQNAgentWithExplorationAndReplay(DQNAgentWithExploration):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgentWithExplorationAndReplay, self).__init__(state_size, action_size)\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 64\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=5000)\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))        \n",
    "\n",
    "    \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        # pick samples randomly from replay memory (using batch_size)\n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        target_val = self.model.predict(update_target)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            # TODO\n",
    "           \n",
    "        # and do the model fit!\n",
    "        (\"loss :\", self.model.train_on_batch(update_input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0,\t Total reward 161.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 20,\t Total reward 9.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 40,\t Total reward 146.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 60,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 80,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 100,\t Total reward 171.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 120,\t Total reward 11.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 140,\t Total reward 133.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 160,\t Total reward 157.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 180,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 200,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 220,\t Total reward 9.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 240,\t Total reward 9.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 260,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 280,\t Total reward 145.0,\t Memory length 5000,\t Epsilon 0.00\n",
      "Episode 300,\t Total reward 199.0,\t Memory length 5000,\t Epsilon 0.00\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DQNAgentWithExplorationAndReplay(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 301\n",
    "done = False\n",
    "\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        agent.train_model()\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {},\\t Total reward {},\\t Memory length {},\\t Epsilon {:0.2f}\".format(i, total_reward, len(agent.memory), agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN Agent with Exploration and Experience Replay\n",
    "\n",
    "Now we train two identical networks. We keep frozen for some timesteps the network that is in charge of evaluation (i.e that is used to compute targets).\n",
    "Note that you can find some variants where the target network is updated at each timestep but with a small fraction of the difference with the policy network.\n",
    "\n",
    "Reference: [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "* Implement Double DQN, this is should reduce the variance of predicted Q-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DoubleDQNAgentWithExplorationAndReplay(DQNAgentWithExplorationAndReplay):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DoubleDQNAgentWithExplorationAndReplay, self).__init__(state_size, action_size)\n",
    "        # TODO: initialize a second model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        # TODO: copy weights from the model used for action selection to the model used for computing targets       \n",
    "\n",
    "    \n",
    "    def train_model(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.state_size))\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        # pick samples randomly from replay memory \n",
    "        for i in range(self.batch_size):\n",
    "            update_input[i] = mini_batch[i][0]\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            update_target[i] = mini_batch[i][3]\n",
    "            done.append(mini_batch[i][4])\n",
    "\n",
    "        target = self.model.predict(update_input)\n",
    "        # TODO: compute targets (in target_val)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            # approx Q Learning\n",
    "            if done[i]:\n",
    "                target[i][action[i]] = reward[i]\n",
    "            else:\n",
    "                target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_val[i]))\n",
    "\n",
    "        # and do the model fit!\n",
    "        self.model.train_on_batch(update_input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Episode 1,\t Total reward 21.0,\t Memory length 31,\t Epsilon 0.86\n",
      "Episode 21,\t Total reward 16.0,\t Memory length 305,\t Epsilon 0.22\n",
      "Episode 41,\t Total reward 32.0,\t Memory length 604,\t Epsilon 0.05\n",
      "Episode 61,\t Total reward 75.0,\t Memory length 1875,\t Epsilon 0.01\n",
      "Episode 81,\t Total reward 159.0,\t Memory length 3697,\t Epsilon 0.01\n",
      "Episode 101,\t Total reward 156.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 121,\t Total reward 165.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 141,\t Total reward 162.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 161,\t Total reward 179.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 181,\t Total reward 176.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 201,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 221,\t Total reward 145.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 241,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 261,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 281,\t Total reward 110.0,\t Memory length 5000,\t Epsilon 0.01\n",
      "Episode 301,\t Total reward 200.0,\t Memory length 5000,\t Epsilon 0.01\n"
     ]
    }
   ],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "agent = DoubleDQNAgentWithExplorationAndReplay(state_size, env.action_space.n)\n",
    "\n",
    "episode_count = 301\n",
    "done = False\n",
    "\n",
    "# TODO: add the update of the target model at the end of each episode\n",
    "for i in range(episode_count):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.append_sample(state, action, reward, next_state, done)\n",
    "        total_reward += reward\n",
    "        agent.train_model()\n",
    "        agent.update_epsilon()\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            if i % 20 == 0:\n",
    "                print(\"Episode {},\\t Total reward {},\\t Memory length {},\\t Epsilon {:0.2f}\".format(i, total_reward, len(agent.memory), agent.epsilon))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the actual performance of the policy, we should set $\\varepsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total reward 200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x152a4e978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.epsilon = 0\n",
    "display_policy(env, agent, episodes=1, add_batch_dim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dueling DQN \n",
    "\n",
    "Dueling network contains two separate estimators: one for the state value function and one for the state-dependent action advantage function. This should reduce the variance of Q-values estimation.\n",
    "\n",
    "Reference: [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)\n",
    "\n",
    "## Task 5\n",
    "\n",
    "* Implement Dueling DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More ideas\n",
    "\n",
    "* Compute averaged curves over several initialisations for each algorithm\n",
    "* Shape the reward in order to keep the cart close to its initial position\n",
    "* Do not sample uniformly in the memory for the batches using [prioritized replay](https://arxiv.org/pdf/1511.05952.pdf)\n",
    "* (Painfull) learn a policy with a bar starting from the bottom position\n",
    "* [Visualize](http://arxiv.org/abs/1412.6806) important areas of the pictures for taking the decision using saliency maps computed by guided backpropagation\n",
    "* Try policy gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spoiler\n",
    "Some working parameters for the network: \n",
    "* gamma = 0.995\n",
    "* learning_rate = 0.001\n",
    "* Exploration by $\\varepsilon$-greddy with multiplicative decay and minimal value \n",
    "  ```\n",
    "  self.epsilon = 1.0\n",
    "  self.epsilon_decay = 0.995\n",
    "  self.epsilon_min = 0.01\n",
    "  ```\n",
    "* Structure of the network: 2 couches de 24 neurones compltement connectes avec activation relu, Cost MSE et optimis par descente de gradient Adam:\n",
    "  ```\n",
    "  model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "  model.add(Dense(24, activation='relu'))\n",
    "  model.add(Dense(self.action_size, activation='linear'))\n",
    "  model.summary()\n",
    "  model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "  ```\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
